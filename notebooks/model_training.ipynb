{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "798a2e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRFRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b81718c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Store</th>\n",
       "      <th>Date</th>\n",
       "      <th>Weekly_Sales</th>\n",
       "      <th>Holiday_Flag</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Fuel_Price</th>\n",
       "      <th>CPI</th>\n",
       "      <th>Unemployment</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>week</th>\n",
       "      <th>Quarter</th>\n",
       "      <th>Trend</th>\n",
       "      <th>Rolling_mean_5</th>\n",
       "      <th>Rolling_mean_10</th>\n",
       "      <th>Lag_1</th>\n",
       "      <th>Lag_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2010-02-05</td>\n",
       "      <td>1643690.90</td>\n",
       "      <td>0</td>\n",
       "      <td>42.31</td>\n",
       "      <td>2.572</td>\n",
       "      <td>211.096358</td>\n",
       "      <td>8.106</td>\n",
       "      <td>2010</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1125378.184</td>\n",
       "      <td>1159822.43</td>\n",
       "      <td>1643690.90</td>\n",
       "      <td>1643690.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>2010-02-05</td>\n",
       "      <td>2193048.75</td>\n",
       "      <td>0</td>\n",
       "      <td>54.34</td>\n",
       "      <td>2.962</td>\n",
       "      <td>126.442065</td>\n",
       "      <td>9.765</td>\n",
       "      <td>2010</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1125378.184</td>\n",
       "      <td>1159822.43</td>\n",
       "      <td>1643690.90</td>\n",
       "      <td>1643690.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37</td>\n",
       "      <td>2010-02-05</td>\n",
       "      <td>536006.73</td>\n",
       "      <td>0</td>\n",
       "      <td>45.97</td>\n",
       "      <td>2.572</td>\n",
       "      <td>209.852966</td>\n",
       "      <td>8.554</td>\n",
       "      <td>2010</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1125378.184</td>\n",
       "      <td>1159822.43</td>\n",
       "      <td>2193048.75</td>\n",
       "      <td>1643690.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17</td>\n",
       "      <td>2010-02-05</td>\n",
       "      <td>789036.02</td>\n",
       "      <td>0</td>\n",
       "      <td>23.11</td>\n",
       "      <td>2.666</td>\n",
       "      <td>126.442065</td>\n",
       "      <td>6.548</td>\n",
       "      <td>2010</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1125378.184</td>\n",
       "      <td>1159822.43</td>\n",
       "      <td>536006.73</td>\n",
       "      <td>2193048.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30</td>\n",
       "      <td>2010-02-05</td>\n",
       "      <td>465108.52</td>\n",
       "      <td>0</td>\n",
       "      <td>39.05</td>\n",
       "      <td>2.572</td>\n",
       "      <td>210.752605</td>\n",
       "      <td>8.324</td>\n",
       "      <td>2010</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1125378.184</td>\n",
       "      <td>1159822.43</td>\n",
       "      <td>789036.02</td>\n",
       "      <td>536006.73</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Store        Date  Weekly_Sales  Holiday_Flag  Temperature  Fuel_Price  \\\n",
       "0      1  2010-02-05    1643690.90             0        42.31       2.572   \n",
       "1     10  2010-02-05    2193048.75             0        54.34       2.962   \n",
       "2     37  2010-02-05     536006.73             0        45.97       2.572   \n",
       "3     17  2010-02-05     789036.02             0        23.11       2.666   \n",
       "4     30  2010-02-05     465108.52             0        39.05       2.572   \n",
       "\n",
       "          CPI  Unemployment  year  month  week  Quarter  Trend  \\\n",
       "0  211.096358         8.106  2010      2     5        1      1   \n",
       "1  126.442065         9.765  2010      2     5        1      2   \n",
       "2  209.852966         8.554  2010      2     5        1      3   \n",
       "3  126.442065         6.548  2010      2     5        1      4   \n",
       "4  210.752605         8.324  2010      2     5        1      5   \n",
       "\n",
       "   Rolling_mean_5  Rolling_mean_10       Lag_1       Lag_2  \n",
       "0     1125378.184       1159822.43  1643690.90  1643690.90  \n",
       "1     1125378.184       1159822.43  1643690.90  1643690.90  \n",
       "2     1125378.184       1159822.43  2193048.75  1643690.90  \n",
       "3     1125378.184       1159822.43   536006.73  2193048.75  \n",
       "4     1125378.184       1159822.43   789036.02   536006.73  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/walmart_ml_data.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3126a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Test Split Completed!\n",
      "Training Set: 4826 records\n",
      "Testing Set:  1609 records\n",
      "Total Records: 6435\n"
     ]
    }
   ],
   "source": [
    "X = df.drop(columns=['Date', 'Weekly_Sales'])\n",
    "y = df['Weekly_Sales']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, shuffle=False, random_state=42)\n",
    "\n",
    "print(\"Train Test Split Completed!\")\n",
    "print(f\"Training Set: {len(X_train)} records\")\n",
    "print(f\"Testing Set:  {len(X_test)} records\")\n",
    "print(f\"Total Records: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95982313",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, root_mean_squared_error, r2_score\n",
    "\n",
    "def evaluate_log_and_real(y_true_log, y_pred_log):\n",
    "    # log-space metrics (model-centric)\n",
    "    mae_log = mean_absolute_error(y_true_log, y_pred_log)\n",
    "    rmse_log = root_mean_squared_error(y_true_log, y_pred_log)\n",
    "    r2 = r2_score(y_true_log, y_pred_log)\n",
    "\n",
    "    # real-space metrics (business-centric)\n",
    "    y_true_real = np.expm1(y_true_log)\n",
    "    y_pred_real = np.expm1(y_pred_log)\n",
    "\n",
    "    mae_real = mean_absolute_error(y_true_real, y_pred_real)\n",
    "    rmse_real = root_mean_squared_error(y_true_real, y_pred_real)\n",
    "\n",
    "    return mae_log, rmse_log, mae_real, rmse_real, r2\n",
    "\n",
    "def evaluate(y_true, y_pred):\n",
    "    MAE = mean_absolute_error(y_true, y_pred)\n",
    "    RMSE = root_mean_squared_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "    return MAE, RMSE, r2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68457861",
   "metadata": {},
   "source": [
    "## Bulk training multiple models\n",
    "\n",
    "Models trained are\n",
    "1. Linear Regression\n",
    "2. Light GBM Regressor\n",
    "3. XGB Regressor\n",
    "4. Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35bb840b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression\n",
      "Model performance for Training set\n",
      "- Root Mean Squared Error: 433883.7255\n",
      "- Mean Absolute Error: 349396.9955\n",
      "- R2 Score: 0.4273\n",
      "----------------------------------\n",
      "Model performance for Test set\n",
      "- Root Mean Squared Error: 416855.3518\n",
      "- Mean Absolute Error: 341187.2975\n",
      "- R2 Score: 0.3960\n",
      "===================================\n",
      "\n",
      "\n",
      "[LightGBM] [Info] Total Bins 2401\n",
      "[LightGBM] [Info] Number of data points in the train set: 4826, number of used features: 15\n",
      "[LightGBM] [Info] Start training from score 1049130.508026\n",
      "LightGBM\n",
      "Model performance for Training set\n",
      "- Root Mean Squared Error: 75688.8635\n",
      "- Mean Absolute Error: 52552.8273\n",
      "- R2 Score: 0.9826\n",
      "----------------------------------\n",
      "Model performance for Test set\n",
      "- Root Mean Squared Error: 178286.4563\n",
      "- Mean Absolute Error: 109903.0954\n",
      "- R2 Score: 0.8895\n",
      "===================================\n",
      "\n",
      "\n",
      "XGB Regressor\n",
      "Model performance for Training set\n",
      "- Root Mean Squared Error: 262383.7743\n",
      "- Mean Absolute Error: 181571.2011\n",
      "- R2 Score: 0.7906\n",
      "----------------------------------\n",
      "Model performance for Test set\n",
      "- Root Mean Squared Error: 282467.5875\n",
      "- Mean Absolute Error: 203838.1186\n",
      "- R2 Score: 0.7227\n",
      "===================================\n",
      "\n",
      "\n",
      "Random Forest Regressor\n",
      "Model performance for Training set\n",
      "- Root Mean Squared Error: 58103.1427\n",
      "- Mean Absolute Error: 29849.8605\n",
      "- R2 Score: 0.9897\n",
      "----------------------------------\n",
      "Model performance for Test set\n",
      "- Root Mean Squared Error: 189618.5867\n",
      "- Mean Absolute Error: 118268.7884\n",
      "- R2 Score: 0.8750\n",
      "===================================\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    \"LinearRegression\": LinearRegression(),\n",
    "    \"LightGBM\": LGBMRegressor(force_row_wise=True),\n",
    "    \"XGB Regressor\": XGBRFRegressor(),\n",
    "    \"Random Forest Regressor\": RandomForestRegressor()\n",
    "}\n",
    "\n",
    "for i in range(len(list(models.values()))):\n",
    "    model = list(models.values())[i]\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "\n",
    "    train_mae, train_rmse, train_r2 = evaluate(y_train, y_train_pred)\n",
    "    test_mae, test_rmse, test_t2 = evaluate(y_test, y_test_pred)\n",
    "\n",
    "    print(list(models.keys())[i])\n",
    "    \n",
    "    print('Model performance for Training set')\n",
    "    print(\"- Root Mean Squared Error: {:.4f}\".format(train_rmse))\n",
    "    print(\"- Mean Absolute Error: {:.4f}\".format(train_mae))\n",
    "    print(\"- R2 Score: {:.4f}\".format(train_r2))\n",
    "\n",
    "    print('----------------------------------')\n",
    "    \n",
    "    print('Model performance for Test set')\n",
    "    print(\"- Root Mean Squared Error: {:.4f}\".format(test_rmse))\n",
    "    print(\"- Mean Absolute Error: {:.4f}\".format(test_mae))\n",
    "    print(\"- R2 Score: {:.4f}\".format(test_t2))\n",
    "    \n",
    "    print('='*35)\n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ff25d1",
   "metadata": {},
   "source": [
    "### Inference from bulk training results\n",
    "\n",
    "Linear Regression performs the worst with R2 score of 0.42 on test set. The evidently shows it is not able to model the non linear relationships in the data.\n",
    "\n",
    "Light BGM performs the best on both train and test wih R2 score of **0.98** (train) and **0.88** (test). It generalizes well on unseen data with no evidence of major overfitting. This result is before hyperparameter tuning of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add793c1",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning of Light GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5a516d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-01-05 16:21:10,158] A new study created in memory with name: no-name-16b0c12a-c9d4-4600-a761-b966c8d2ea5b\n",
      "Best trial: 0. Best value: 250454:  10%|█         | 1/10 [00:10<01:37, 10.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2026-01-05 16:21:20,936] Trial 0 finished with value: 250454.27644742123 and parameters: {'n_estimators': 812, 'num_leaves': 123, 'max_bin': 393}. Best is trial 0 with value: 250454.27644742123.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 1. Best value: 239537:  20%|██        | 2/10 [00:18<01:09,  8.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2026-01-05 16:21:28,176] Trial 1 finished with value: 239537.26407791692 and parameters: {'n_estimators': 1238, 'num_leaves': 33, 'max_bin': 162}. Best is trial 1 with value: 239537.26407791692.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 1. Best value: 239537:  30%|███       | 3/10 [00:23<00:51,  7.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2026-01-05 16:21:33,979] Trial 2 finished with value: 257058.0092764317 and parameters: {'n_estimators': 210, 'num_leaves': 113, 'max_bin': 341}. Best is trial 1 with value: 239537.26407791692.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 227357:  40%|████      | 4/10 [00:29<00:39,  6.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2026-01-05 16:21:39,435] Trial 3 finished with value: 227357.22964475994 and parameters: {'n_estimators': 1446, 'num_leaves': 18, 'max_bin': 488}. Best is trial 3 with value: 227357.22964475994.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 227357:  50%|█████     | 5/10 [00:33<00:29,  5.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2026-01-05 16:21:44,096] Trial 4 finished with value: 239147.6077863528 and parameters: {'n_estimators': 1682, 'num_leaves': 39, 'max_bin': 172}. Best is trial 3 with value: 227357.22964475994.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 227357:  60%|██████    | 6/10 [00:35<00:17,  4.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2026-01-05 16:21:45,671] Trial 5 finished with value: 259969.60638597785 and parameters: {'n_estimators': 448, 'num_leaves': 50, 'max_bin': 310}. Best is trial 3 with value: 227357.22964475994.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 227357:  70%|███████   | 7/10 [00:38<00:12,  4.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2026-01-05 16:21:48,902] Trial 6 finished with value: 253961.89497752613 and parameters: {'n_estimators': 921, 'num_leaves': 48, 'max_bin': 345}. Best is trial 3 with value: 227357.22964475994.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 227357:  80%|████████  | 8/10 [00:39<00:06,  3.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2026-01-05 16:21:50,084] Trial 7 finished with value: 252520.53366302437 and parameters: {'n_estimators': 365, 'num_leaves': 49, 'max_bin': 246}. Best is trial 3 with value: 227357.22964475994.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 227357:  90%|█████████ | 9/10 [00:44<00:03,  3.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2026-01-05 16:21:55,128] Trial 8 finished with value: 244954.71365207463 and parameters: {'n_estimators': 966, 'num_leaves': 104, 'max_bin': 180}. Best is trial 3 with value: 227357.22964475994.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 227357: 100%|██████████| 10/10 [00:49<00:00,  4.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2026-01-05 16:21:59,517] Trial 9 finished with value: 243157.8969265549 and parameters: {'n_estimators': 1077, 'num_leaves': 82, 'max_bin': 118}. Best is trial 3 with value: 227357.22964475994.\n",
      "Best value: 227357.22964475994\n",
      "Best params: {'n_estimators': 1446, 'num_leaves': 18, 'max_bin': 488}\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000555 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4275\n",
      "[LightGBM] [Info] Number of data points in the train set: 4826, number of used features: 15\n",
      "[LightGBM] [Info] Start training from score 1049130.508026\n",
      "Test RMSE: 166302.53733724545\n",
      "Test R2: 0.9038688294928182\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['lgb_best_model.pkl']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import optuna\n",
    "from sklearn.model_selection import cross_val_score, TimeSeriesSplit\n",
    "from optuna.samplers import TPESampler\n",
    "from optuna.pruners import MedianPruner\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import joblib\n",
    "\n",
    "cv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "def objective(trial):\n",
    "    parameters = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 2000),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 16, 128),\n",
    "        \"max_bin\": trial.suggest_int(\"max_bin\", 100, 500),\n",
    "    }\n",
    "\n",
    "    model =LGBMRegressor(**parameters)\n",
    "\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=cv, scoring=\"neg_root_mean_squared_error\", n_jobs=-1)\n",
    "\n",
    "    return -scores.mean()\n",
    "\n",
    "sampler = TPESampler(seed=42)\n",
    "pruner = MedianPruner(n_warmup_steps=5)\n",
    "study = optuna.create_study(direction=\"minimize\", sampler=sampler, pruner=pruner)\n",
    "study.optimize(objective, n_trials=10, timeout=None, show_progress_bar=True)\n",
    "\n",
    "print(\"Best value:\", study.best_value)\n",
    "print(\"Best params:\", study.best_params)\n",
    "\n",
    "best_model = LGBMRegressor(**study.best_params, random_state=42, n_jobs=-1)\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = best_model.predict(X_test)\n",
    "rmse = root_mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"Test RMSE:\", rmse)\n",
    "print(\"Test R2:\", r2)\n",
    "\n",
    "# save study and model\n",
    "joblib.dump(study, \"../runs/optuna_lgb_study.pkl\")\n",
    "joblib.dump(best_model, \"../models/lgb_best_model.pkl\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1b058c",
   "metadata": {},
   "source": [
    "This concludes, Linear Regression performs the worst while the Light GBM and Random Forest performs the best"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
